import cv2
import time
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from ultralytics import YOLO  # –î–ª—è YOLOv8
import supervision as sv  # –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ü–∏–π


# =================== 1. –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò YOLO ===================
def load_yolo_model(model_name='yolov8n.pt'):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLO –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤
    –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt
    """
    print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (CPU/GPU)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å YOLO
    model = YOLO(model_name)
    model.to(device)

    # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤ COCO
    class_names = model.names

    print(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω–æ –∫–ª–∞—Å—Å–æ–≤: {len(class_names)}")
    return model, class_names, device


# =================== 2. –§–£–ù–ö–¶–ò–ò –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò ===================
def detect_objects_in_frame(model, frame, device, confidence_threshold=0.5):
    """
    –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞–¥—Ä–µ
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR –≤ RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

    # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é
    results = model(frame_rgb, conf=confidence_threshold, device=device)

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    detections = []

    if results and len(results) > 0:
        result = results[0]

        # –ü–æ–ª—É—á–∞–µ–º bounding boxes, confidence scores –∏ class IDs
        boxes = result.boxes.xyxy.cpu().numpy()  # [x1, y1, x2, y2]
        confidences = result.boxes.conf.cpu().numpy()
        class_ids = result.boxes.cls.cpu().numpy().astype(int)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π
        for box, confidence, class_id in zip(boxes, confidences, class_ids):
            x1, y1, x2, y2 = box
            detections.append({
                'bbox': [x1, y1, x2, y2],
                'confidence': float(confidence),
                'class_id': int(class_id),
                'class_name': model.names[class_id],
                'center': [(x1 + x2) / 2, (y1 + y2) / 2],
                'width': x2 - x1,
                'height': y2 - y1
            })

    return detections, frame_rgb


def draw_detections(frame, detections, show_labels=True, show_confidence=True):
    """
    –†–∏—Å—É–µ—Ç bounding boxes –∏ –º–µ—Ç–∫–∏ –Ω–∞ –∫–∞–¥—Ä–µ
    """
    frame_with_detections = frame.copy()

    # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
    colors = {
        'person': (0, 255, 0),  # –ó–µ–ª–µ–Ω—ã–π
        'car': (255, 0, 0),  # –°–∏–Ω–∏–π
        'truck': (0, 0, 255),  # –ö—Ä–∞—Å–Ω—ã–π
        'bus': (255, 255, 0),  # –ì–æ–ª—É–±–æ–π
        'bicycle': (255, 0, 255),  # –§–∏–æ–ª–µ—Ç–æ–≤—ã–π
        'motorcycle': (0, 255, 255)  # –ñ–µ–ª—Ç—ã–π
    }

    for detection in detections:
        x1, y1, x2, y2 = detection['bbox']
        class_name = detection['class_name']
        confidence = detection['confidence']

        # –í—ã–±–∏—Ä–∞–µ–º —Ü–≤–µ—Ç –¥–ª—è –∫–ª–∞—Å—Å–∞
        color = colors.get(class_name, (255, 255, 255))  # –ë–µ–ª—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –†–∏—Å—É–µ–º bounding box
        cv2.rectangle(frame_with_detections,
                      (int(x1), int(y1)),
                      (int(x2), int(y2)),
                      color, 2)

        # –†–∏—Å—É–µ–º –º–µ—Ç–∫—É —Å –∫–ª–∞—Å—Å–æ–º –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if show_labels:
            label = f"{class_name}"
            if show_confidence:
                label += f" {confidence:.2f}"

            # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(frame_with_detections,
                          (int(x1), int(y1) - text_size[1] - 10),
                          (int(x1) + text_size[0], int(y1)),
                          color, -1)

            # –¢–µ–∫—Å—Ç
            cv2.putText(frame_with_detections, label,
                        (int(x1), int(y1) - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

        # –†–∏—Å—É–µ–º —Ü–µ–Ω—Ç—Ä –æ–±—ä–µ–∫—Ç–∞
        center_x, center_y = detection['center']
        cv2.circle(frame_with_detections,
                   (int(center_x), int(center_y)),
                   3, color, -1)

    return frame_with_detections


# =================== 3. –§–£–ù–ö–¶–ò–ò –°–¢–ê–¢–ò–°–¢–ò–ö–ò –ò –ê–ù–ê–õ–ò–ó–ê ===================
def analyze_detections(detections, frame_width, frame_height):
    """
    –ê–Ω–∞–ª–∏–∑ –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    """
    analysis = {
        'total_objects': len(detections),
        'by_class': {},
        'in_zones': {'center': 0, 'left': 0, 'right': 0, 'top': 0, 'bottom': 0},
        'average_confidence': 0,
        'largest_object': None,
        'smallest_object': None
    }

    if not detections:
        return analysis

    total_confidence = 0
    max_area = 0
    min_area = float('inf')

    for detection in detections:
        class_name = detection['class_name']
        confidence = detection['confidence']

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        analysis['by_class'][class_name] = analysis['by_class'].get(class_name, 0) + 1

        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        total_confidence += confidence

        # –ê–Ω–∞–ª–∏–∑ –∑–æ–Ω
        center_x, center_y = detection['center']

        if center_x < frame_width * 0.33:
            analysis['in_zones']['left'] += 1
        elif center_x > frame_width * 0.66:
            analysis['in_zones']['right'] += 1

        if center_y < frame_height * 0.33:
            analysis['in_zones']['top'] += 1
        elif center_y > frame_height * 0.66:
            analysis['in_zones']['bottom'] += 1

        if (frame_width * 0.33 < center_x < frame_width * 0.66 and
                frame_height * 0.33 < center_y < frame_height * 0.66):
            analysis['in_zones']['center'] += 1

        # –†–∞–∑–º–µ—Ä –æ–±—ä–µ–∫—Ç–∞
        area = detection['width'] * detection['height']
        if area > max_area:
            max_area = area
            analysis['largest_object'] = detection
        if area < min_area:
            min_area = area
            analysis['smallest_object'] = detection

    analysis['average_confidence'] = total_confidence / len(detections)

    return analysis


# =================== 4. –û–°–ù–û–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –° –î–í–£–ú–Ø –û–ö–ù–ê–ú–ò ===================
def real_time_object_detection_with_dual_window(camera_id=0, model_name='yolov8n.pt'):
    """–ó–∞–ø—É—Å–∫ –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å –¥–≤—É–º—è –æ–∫–Ω–∞–º–∏"""

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏...")
    model, class_names, device = load_yolo_model(model_name)

    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∫–∞–º–µ—Ä—É
    cap = cv2.VideoCapture(camera_id)
    if not cap.isOpened():
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–º–µ—Ä–µ")
        return

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–∞–º–µ—Ä—ã
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)

    # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–∞
    cv2.namedWindow('üì∑ –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ', cv2.WINDOW_NORMAL)
    cv2.namedWindow('üîç –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤', cv2.WINDOW_NORMAL)
    cv2.namedWindow('üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', cv2.WINDOW_NORMAL)

    # –†–∞–∑–º–µ—â–∞–µ–º –æ–∫–Ω–∞
    cv2.resizeWindow('üì∑ –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ', 640, 480)
    cv2.resizeWindow('üîç –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤', 640, 480)
    cv2.resizeWindow('üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 640, 480)

    cv2.moveWindow('üì∑ –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ', 100, 100)
    cv2.moveWindow('üîç –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤', 750, 100)
    cv2.moveWindow('üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 1400, 100)

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ—Ç–µ–∫—Ü–∏–∏
    confidence_threshold = 0.5
    detection_interval = 0.1  # 10 FPS –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏

    # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è
    last_detection_time = time.time()
    frame_count = 0
    start_time = time.time()

    # –ë—É—Ñ–µ—Ä –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    detection_history = []

    print("\n" + "=" * 80)
    print("–°–ò–°–¢–ï–ú–ê –î–ï–¢–ï–ö–¶–ò–ò –û–ë–™–ï–ö–¢–û–í –í –†–ï–ê–õ–¨–ù–û–ú –í–†–ï–ú–ï–ù–ò")
    print("=" * 80)
    print("–û–∫–Ω–∞:")
    print("  1. üì∑ –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ - –ü—Ä—è–º–æ–π —ç—Ñ–∏—Ä —Å –∫–∞–º–µ—Ä—ã")
    print("  2. üîç –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è")
    print("  3. üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ - –ê–Ω–∞–ª–∏–∑ –¥–µ—Ç–µ–∫—Ü–∏–π")
    print("\n–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:")
    print("  'q' - –í—ã—Ö–æ–¥")
    print("  's' - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–¥—Ä —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π")
    print("  '+' - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
    print("  '-' - –£–º–µ–Ω—å—à–∏—Ç—å –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
    print("  '1' - –ü–æ–∫–∞–∑–∞—Ç—å/—Å–∫—Ä—ã—Ç—å –º–µ—Ç–∫–∏")
    print("  '2' - –ü–æ–∫–∞–∑–∞—Ç—å/—Å–∫—Ä—ã—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
    print("  '3' - –í–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ç—Ä–µ–∫–∏–Ω–≥ —Ü–µ–Ω—Ç—Ä–∞")
    print("  'c' - –°–±—Ä–æ—Å–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É")
    print("=" * 80)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
    show_labels = True
    show_confidence = True
    show_center_points = True

    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
    while True:
        ret, frame = cap.read()
        if not ret:
            print("–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–∞")
            break

        frame_count += 1
        current_time = time.time()

        # ============ –û–ö–ù–û 1: –ò–°–•–û–î–ù–û–ï –í–ò–î–ï–û ============
        original_display = frame.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        fps = frame_count / (current_time - start_time)
        cv2.putText(original_display, f"FPS: {fps:.1f}", (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        cv2.putText(original_display, f"–ü–æ—Ä–æ–≥: {confidence_threshold:.2f}", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        # –†–∏—Å—É–µ–º –∑–æ–Ω—ã –∞–Ω–∞–ª–∏–∑–∞
        h, w = frame.shape[:2]
        cv2.rectangle(original_display, (int(w * 0.33), int(h * 0.33)),
                      (int(w * 0.66), int(h * 0.66)), (0, 255, 255), 1)

        cv2.imshow('üì∑ –ò—Å—Ö–æ–¥–Ω–æ–µ –≤–∏–¥–µ–æ', original_display)

        # ============ –í–´–ü–û–õ–ù–ï–ù–ò–ï –î–ï–¢–ï–ö–¶–ò–ò ============
        if current_time - last_detection_time >= detection_interval:
            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
            detections, processed_frame = detect_objects_in_frame(
                model, frame, device, confidence_threshold
            )

            # –ê–Ω–∞–ª–∏–∑ –¥–µ—Ç–µ–∫—Ü–∏–π
            analysis = analyze_detections(detections, w, h)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            detection_history.append({
                'time': current_time,
                'detections': detections,
                'analysis': analysis
            })

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(detection_history) > 100:
                detection_history.pop(0)

            last_detection_time = current_time

            # ============ –û–ö–ù–û 2: –î–ï–¢–ï–ö–¶–ò–Ø –û–ë–™–ï–ö–¢–û–í ============
            detection_display = draw_detections(
                frame.copy(), detections, show_labels, show_confidence
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤
            cv2.putText(detection_display, f"–û–±—ä–µ–∫—Ç–æ–≤: {len(detections)}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)

            cv2.imshow('üîç –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤', detection_display)

            # ============ –û–ö–ù–û 3: –°–¢–ê–¢–ò–°–¢–ò–ö–ê ============
            stats_display = np.zeros((480, 640, 3), dtype=np.uint8)
            stats_display[:] = (30, 30, 30)  # –¢–µ–º–Ω—ã–π —Ñ–æ–Ω

            y_offset = 40

            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            cv2.putText(stats_display, "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–¢–ï–ö–¶–ò–ò", (20, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            y_offset += 40

            # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            cv2.putText(stats_display, f"–í—Å–µ–≥–æ –æ–±—ä–µ–∫—Ç–æ–≤: {analysis['total_objects']}",
                        (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            y_offset += 30

            cv2.putText(stats_display, f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {analysis['average_confidence']:.2f}",
                        (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
            y_offset += 30

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
            cv2.putText(stats_display, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:",
                        (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
            y_offset += 25

            for class_name, count in analysis['by_class'].items():
                cv2.putText(stats_display, f"  {class_name}: {count}",
                            (30, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_offset += 20

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∑–æ–Ω–∞–º
            y_offset += 10
            cv2.putText(stats_display, "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∑–æ–Ω–∞–º:",
                        (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
            y_offset += 25

            zones = ['center', 'left', 'right', 'top', 'bottom']
            for zone in zones:
                cv2.putText(stats_display, f"  {zone}: {analysis['in_zones'][zone]}",
                            (30, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_offset += 20

            # –ò—Å—Ç–æ—Ä–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π (–≥—Ä–∞—Ñ–∏–∫)
            if len(detection_history) > 1:
                y_offset += 10
                cv2.putText(stats_display, "–¢–µ–Ω–¥–µ–Ω—Ü–∏—è (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∫–∞–¥—Ä–æ–≤):",
                            (20, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
                y_offset += 25

                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ–∏–∫
                recent_counts = [d['analysis']['total_objects']
                                 for d in detection_history[-50:]]
                if recent_counts:
                    max_count = max(recent_counts) if max(recent_counts) > 0 else 1
                    graph_height = 100
                    graph_width = 400
                    graph_x = 100
                    graph_y = y_offset + 50

                    # –†–∏—Å—É–µ–º –æ—Å–∏
                    cv2.rectangle(stats_display, (graph_x, graph_y),
                                  (graph_x + graph_width, graph_y + graph_height),
                                  (100, 100, 100), 1)

                    # –†–∏—Å—É–µ–º –≥—Ä–∞—Ñ–∏–∫
                    points = []
                    for i, count in enumerate(recent_counts):
                        x = graph_x + int(i * graph_width / len(recent_counts))
                        y = graph_y + graph_height - int((count / max_count) * graph_height)
                        points.append((x, y))

                    for i in range(len(points) - 1):
                        cv2.line(stats_display, points[i], points[i + 1],
                                 (0, 255, 255), 2)

            cv2.imshow('üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', stats_display)

        # ============ –û–ë–†–ê–ë–û–¢–ö–ê –ö–õ–ê–í–ò–® ============
        key = cv2.waitKey(1) & 0xFF

        if key == ord('q'):
            break
        elif key == ord('s'):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–¥—Ä —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            cv2.imwrite(f'detection_{timestamp}.jpg', detection_display)
            print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: detection_{timestamp}.jpg")
        elif key == ord('+'):
            confidence_threshold = min(0.95, confidence_threshold + 0.05)
            print(f"–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {confidence_threshold:.2f}")
        elif key == ord('-'):
            confidence_threshold = max(0.1, confidence_threshold - 0.05)
            print(f"–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {confidence_threshold:.2f}")
        elif key == ord('1'):
            show_labels = not show_labels
            print(f"–ú–µ—Ç–∫–∏: {'–í–ö–õ' if show_labels else '–í–´–ö–õ'}")
        elif key == ord('2'):
            show_confidence = not show_confidence
            print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {'–í–ö–õ' if show_confidence else '–í–´–ö–õ'}")
        elif key == ord('3'):
            show_center_points = not show_center_points
            print(f"–¢–æ—á–∫–∏ —Ü–µ–Ω—Ç—Ä–∞: {'–í–ö–õ' if show_center_points else '–í–´–ö–õ'}")
        elif key == ord('c'):
            detection_history = []
            print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–±—Ä–æ—à–µ–Ω–∞")

    # ============ –ó–ê–í–ï–†–®–ï–ù–ò–ï ============
    total_time = time.time() - start_time

    print("\n" + "=" * 80)
    print("–§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("=" * 80)
    print(f"–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {frame_count}")
    print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —Å–µ–∫")
    print(f"–°—Ä–µ–¥–Ω–∏–π FPS: {frame_count / total_time:.1f}")

    if detection_history:
        total_detections = sum([len(d['detections']) for d in detection_history])
        avg_detections = total_detections / len(detection_history)
        print(f"–°—Ä–µ–¥–Ω–µ–µ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä: {avg_detections:.2f}")

    # –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
    cap.release()
    cv2.destroyAllWindows()


# =================== 5. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===================
def detect_in_image(image_path, model_name='yolov8n.pt', confidence_threshold=0.5):
    """–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, class_names, device = load_yolo_model(model_name)

    # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    image = cv2.imread(image_path)
    if image is None:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
        return

    # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é
    detections, _ = detect_objects_in_frame(model, image, device, confidence_threshold)

    # –†–∏—Å—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏
    result_image = draw_detections(image, detections)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    cv2.imshow('–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏', result_image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"\n–ù–∞–π–¥–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(detections)}")
    for i, det in enumerate(detections, 1):
        print(f"{i}. {det['class_name']} ({det['confidence']:.2f}) - "
              f"[{det['bbox'][0]:.0f}, {det['bbox'][1]:.0f}, {det['bbox'][2]:.0f}, {det['bbox'][3]:.0f}]")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    output_path = image_path.replace('.', '_detected.')
    cv2.imwrite(output_path, result_image)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")


# =================== 6. –ó–ê–ü–£–°–ö –ü–†–û–ì–†–ê–ú–ú–´ ===================
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='–î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏')
    parser.add_argument('--camera', type=int, default=0, help='ID –∫–∞–º–µ—Ä—ã')
    parser.add_argument('--model', type=str, default='yolov8n.pt',
                        choices=['yolov8n.pt', 'yolov8s.pt', 'yolov8m.pt',
                                 'yolov8l.pt', 'yolov8x.pt'],
                        help='–ú–æ–¥–µ–ª—å YOLO –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è')
    parser.add_argument('--image', type=str, help='–ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏')
    parser.add_argument('--confidence', type=float, default=0.5,
                        help='–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')

    args = parser.parse_args()


    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–∞–º–µ—Ä—ã
    def check_camera():
        for i in range(3):
            cap_test = cv2.VideoCapture(i)
            if cap_test.isOpened():
                print(f"–ù–∞–π–¥–µ–Ω–∞ –∫–∞–º–µ—Ä–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º {i}")
                cap_test.release()
                return i
            cap_test.release()
        return 0


    if args.image:
        # –†–µ–∂–∏–º –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        detect_in_image(args.image, args.model, args.confidence)
    else:
        # –†–µ–∂–∏–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
        camera_id = check_camera()
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–º–µ—Ä–∞: {camera_id}")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–æ–¥–µ–ª—å: {args.model}")

        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        try:
            import ultralytics
        except ImportError:
            print("\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:")
            print("pip install ultralytics")
            print("pip install supervision")
            exit(1)

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º—É –¥–µ—Ç–µ–∫—Ü–∏–∏
        real_time_object_detection_with_dual_window(camera_id, args.model)