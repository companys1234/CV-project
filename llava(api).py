# -*- coding: utf-8 -*-
"""llava(api)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rz2Xyl8HQMJo4GVbpqT60wBMdLwKFsr0
"""

from transformers import LlavaForConditionalGeneration, AutoProcessor
from PIL import Image
import torch

# Загрузка модели и процессора
model_id = "llava-hf/llava-1.5-7b-hf"
model = LlavaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")
processor = AutoProcessor.from_pretrained(model_id)

# Загрузка изображения
image = Image.open("/content/клубничке.jpg")  # замените на путь к вашему изображению
prompt = "USER: <image>\nЧто изображено на картинке?\nASSISTANT:"

# Обработка и генерация ответа
inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")
output = model.generate(**inputs, max_new_tokens=200)
response = processor.decode(output[0][2:], skip_special_tokens=True)

print(response)

from transformers import LlavaForConditionalGeneration, AutoProcessor
import torch
from PIL import Image

# Инициализация модели
model_path = "llava-hf/llava-1.5-7b-hf"
model = LlavaForConditionalGeneration.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_path)

# Загрузка изображения
image = Image.open("/content/клубничке.jpg")  # замените на ваше изображение

# Формирование запроса
prompt = "USER: <image>\nОпиши это изображение подробно.\nASSISTANT:"

# Обработка и генерация
inputs = processor(text=prompt, images=image, return_tensors="pt").to("cuda")
output = model.generate(**inputs, max_new_tokens=200)
response = processor.decode(output[0], skip_special_tokens=True)

print("Ответ LLaVA:", response)